{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.optim import Adam\nfrom torch.nn import CrossEntropyLoss\n\nfrom torchvision import datasets, transforms\nfrom torchvision.transforms import ToTensor\nfrom torchvision.io import read_image\nfrom torchvision.datasets.mnist import MNIST\n\nimport numpy as np\nimport os\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nfrom collections import Counter\n\n\n\n\ndef imshow(image, ax=None, title=None, normalize=True):\n    \"\"\"Imshow for Tensor.\"\"\"\n    if ax is None:\n        fig, ax = plt.subplots()\n    image = image.numpy().transpose((1, 2, 0))\n\n    if normalize:\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225])\n        image = std * image + mean\n        image = np.clip(image, 0, 1)\n\n    ax.imshow(image)\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.tick_params(axis='both', length=0)\n    ax.set_xticklabels('')\n    ax.set_yticklabels('')\n\n    return ax","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-13T16:57:59.036344Z","iopub.execute_input":"2022-06-13T16:57:59.037212Z","iopub.status.idle":"2022-06-13T16:58:01.244696Z","shell.execute_reply.started":"2022-06-13T16:57:59.037112Z","shell.execute_reply":"2022-06-13T16:58:01.243722Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([transforms.Resize(154),\n                                 transforms.CenterCrop(154),\n                                 transforms.ToTensor()])\ndataset = datasets.ImageFolder('../input/transformer1dataset/birds/birds', transform=transform)\n\ntrain_size = int(0.8 * len(dataset))\ntest_size = len(dataset) - train_size\ntrain_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])","metadata":{"execution":{"iopub.status.busy":"2022-06-12T09:01:32.550377Z","iopub.execute_input":"2022-06-12T09:01:32.550918Z","iopub.status.idle":"2022-06-12T09:01:33.146527Z","shell.execute_reply.started":"2022-06-12T09:01:32.550877Z","shell.execute_reply":"2022-06-12T09:01:33.145677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T08:56:58.969177Z","iopub.execute_input":"2022-06-12T08:56:58.969539Z","iopub.status.idle":"2022-06-12T08:56:58.974151Z","shell.execute_reply.started":"2022-06-12T08:56:58.969493Z","shell.execute_reply":"2022-06-12T08:56:58.973063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run this to test your data loader\nimages, labels = next(iter(dataloader))\n# helper.imshow(images[0], normalize=False)\nimshow(images[0], normalize=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T09:01:36.095271Z","iopub.execute_input":"2022-06-12T09:01:36.096075Z","iopub.status.idle":"2022-06-12T09:01:36.548151Z","shell.execute_reply.started":"2022-06-12T09:01:36.096038Z","shell.execute_reply":"2022-06-12T09:01:36.54736Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_classes = [dataset.targets[i] for i in train_dataset.indices]\nCounter(train_classes) # if doesn' work: Counter(i.item() for i in train_classes)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T16:18:36.578975Z","iopub.execute_input":"2022-06-12T16:18:36.579442Z","iopub.status.idle":"2022-06-12T16:18:36.602053Z","shell.execute_reply.started":"2022-06-12T16:18:36.579405Z","shell.execute_reply":"2022-06-12T16:18:36.599984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    #Training loop\n    optimizer = Adam(model.parameters(), lr = LR)\n    criterion = CrossEntropyLoss()\n    for epoch in range(N_EPOCHS):\n        train_loss = 0.0\n        for batch in train_loader:\n            x,y = batch\n            y_hat = model(x)\n            loss = criterion(y_hat,y)/len(x)\n            \n            train_loss += loss.item()\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n        print(f\"Epoch {epoch + 1}/{N_EPOCHS} loss: {train_loss: .2f}\")\n\n   #Test loop\n    correct, total = 0,0\n    test_loss = 0.0\n    for batch in test_loader:\n        x,y=batch\n        y_hat = model(x)\n        loss = criterion(y_hat,y)\n        test_loss += loss/len(x)\n        \n        correct += torch.sum(torch.argmax(y_hat, dim=1) == y).item()\n        total += len(x)   \n    print(f\"Test loss: {test_loss:.2f}\")\n    print(f\"Test accuracy: {correct/total*100:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2022-06-12T08:34:10.544313Z","iopub.status.idle":"2022-06-12T08:34:10.545115Z","shell.execute_reply.started":"2022-06-12T08:34:10.544874Z","shell.execute_reply":"2022-06-12T08:34:10.544897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    model.train()\n    for batch, (X,y) in enumerate(dataloader):\n        X, y = X.to(device), y.to(device)\n        \n        #Compute prediction error\n        pred = model(X)\n        loss = loss_fn(pred, y)\n        \n        #Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if batch %100== 0:\n            loss, current = loss.item(), batch*len(X)\n            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\ndef test(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    model.eval()\n    test_loss, correct = 0, 0\n    with torch.no_grad():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n    ","metadata":{"execution":{"iopub.status.busy":"2022-06-13T16:58:33.176393Z","iopub.execute_input":"2022-06-13T16:58:33.177833Z","iopub.status.idle":"2022-06-13T16:58:33.197950Z","shell.execute_reply.started":"2022-06-13T16:58:33.177781Z","shell.execute_reply":"2022-06-13T16:58:33.196750Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def main():\n    transform = transforms.Compose([transforms.Resize(154),\n                                 transforms.CenterCrop(154),\n                                 transforms.ToTensor()])\n    dataset = datasets.ImageFolder('../input/transformer1dataset/birds/birds', transform=transform)\n\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n    \n    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=9075)\n    test_loader = DataLoader(test_dataset, shuffle=False, batch_size=9075)\n    \n    #Model and training options\n    model = BumbleBird((3,154,154), n_patches = 7, hidden_d=4, n_heads=2, out_d=400).to(device)\n    N_EPOCHS = 5 # Number of epochs\n    LR = 0.01 #Learning rate\n    \n    #Training\n    optimizer = Adam(model.parameters(), lr = LR)\n    criterion = CrossEntropyLoss()\n    for t in range(N_EPOCHS):\n        print(f\"Epoch {t+1}\\n-----------------------------------\")\n        train(train_loader, model, criterion, optimizer)\n        test(test_loader, model, criterion)\n    print (\"Done!\")  \n\ndef get_positional_embeddings(sequence_length, d):\n    result = torch.ones(sequence_length, d)\n    for i in range(sequence_length):\n        for j in range(d):\n            result[i][j] = np.sin(i/(10000**(j/d)) if j%2==0 else np.cos(i/(10000**((j-1)/d))))\n    return result.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T17:17:57.204179Z","iopub.execute_input":"2022-06-13T17:17:57.204816Z","iopub.status.idle":"2022-06-13T17:17:57.216851Z","shell.execute_reply.started":"2022-06-13T17:17:57.204776Z","shell.execute_reply":"2022-06-13T17:17:57.215967Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device} device\")\nmain()","metadata":{"execution":{"iopub.status.busy":"2022-06-13T17:18:02.634211Z","iopub.execute_input":"2022-06-13T17:18:02.634614Z","iopub.status.idle":"2022-06-13T17:42:14.059490Z","shell.execute_reply.started":"2022-06-13T17:18:02.634581Z","shell.execute_reply":"2022-06-13T17:42:14.058540Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def test(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    model.eval()\n    test_loss, correct = 0, 0\n    with torch.no_grad():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\") \n    \n    \n# Predictions\nwith torch.no_grad():\n    \n    transform = transforms.Compose([transforms.Resize(154),\n                                 transforms.CenterCrop(154),\n                                 transforms.ToTensor()])\n    pred_dataset = datasets.ImageFolder('../input/transformer1dataset/submission_test', transform=transform)\n    \n    pred_loader = DataLoader(pred_dataset, shuffle=True, batch_size=16)\n    model = BumbleBird((3,154,154), n_patches = 7, hidden_d=4, n_heads=2, out_d=400).to(device)\n    arr = []\n    for X, y in pred_loader:\n        X, y = X.to(device), y.to(device)\n        prediction = model(X)\n        arr = prediction.data.cpu().detach()\n    # write CSV\n    np.savetxt('output.csv', arr)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-12T17:27:20.07254Z","iopub.execute_input":"2022-06-12T17:27:20.072905Z","iopub.status.idle":"2022-06-12T17:27:27.078017Z","shell.execute_reply.started":"2022-06-12T17:27:20.072867Z","shell.execute_reply":"2022-06-12T17:27:27.077007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Multi-head Self Attention (MSA)\nclass MSAtron(nn.Module):\n    def __init__(self, d, n_heads=2):\n        super(MSAtron, self).__init__()\n        self.d = d\n        self.n_heads = n_heads\n        \n        assert d%n_heads == 0, f\"Can't divide dimension {d} into {n_heads} heads\"\n        \n        d_head = int(d/n_heads)\n        self.q_mappings = [nn.Linear(d_head, d_head).to(device) for _ in range(self.n_heads)]\n        self.k_mappings = [nn.Linear(d_head, d_head).to(device) for _ in range(self.n_heads)]\n        self.v_mappings = [nn.Linear(d_head, d_head).to(device) for _ in range(self.n_heads)]\n        self.d_head = d_head\n        self.softmax = nn.Softmax(dim=1)\n        \n    def forward(self, sequences):\n        result =[]\n        \n        for sequence in sequences:\n            seq_result = []\n            for head in range(self.n_heads):\n                q_mapping = self.q_mappings[head]\n                k_mapping = self.k_mappings[head]\n                v_mapping = self.v_mappings[head]\n                \n                seq = sequence[:, head*self.d_head:(head+1)*self.d_head]\n                q,k,v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n                \n                attention = self.softmax(q @ k.T / (self.d_head**0.5))\n                seq_result.append(attention @ v)\n            result.append(torch.hstack(seq_result))\n        return torch.cat([torch.unsqueeze(r,dim=0) for r in result])\n\n#Modelo\nclass BumbleBird(nn.Module):\n    def __init__(self, input_shape, n_patches=7, hidden_d=8, n_heads=2, out_d=10):\n        #Super constructor\n        super(BumbleBird, self).__init__()\n        \n        #Input and patches sizes\n        self.input_shape = input_shape\n        self.n_patches = n_patches\n        self.patch_size = (input_shape[1]/n_patches, input_shape[2]/n_patches)\n        self.input_d = int(input_shape[0]*self.patch_size[0]*self.patch_size[1])\n        self.hidden_d = hidden_d\n        \n        #Linear mapper\n        self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)\n        \n        #Classification Token\n        self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n        \n        #Layer normalization 1\n        self.ln1 = nn.LayerNorm((self.n_patches**2+1, self.hidden_d))\n        \n        #Multihead Self Attention and class token\n        self.msa = MSAtron(self.hidden_d,n_heads)\n        \n        #Layer normalization 2\n        self.ln2 = nn.LayerNorm((self.n_patches**2+1, self.hidden_d))\n        \n        #Encoder MLP\n        self.enc_mlp = nn.Sequential(\n            nn.Linear(self.hidden_d, self.hidden_d),\n            nn.ReLU()\n        )\n        \n        #Classification MLP\n        self.mlp = nn.Sequential(\n            nn.Linear(self.hidden_d, out_d),\n            nn.Softmax(dim=-1)\n        )\n    \n    def forward(self, images):\n        #Divide the image into patches\n        #print(images.shape)\n        n,c,w,h = images.shape\n        patches = images.reshape(n,self.n_patches**2,self.input_d)\n        \n        #Run linear layer for tokenization\n        tokens = self.linear_mapper(patches)\n        \n        #Adding a classification token to the tokens\n        tokens = torch.stack([torch.vstack((self.class_token, tokens[i])) for i in range(len(tokens))])\n        \n        #Positional embedding\n        tokens += get_positional_embeddings(self.n_patches**2+1,self.hidden_d).repeat(n,1,1)\n\n        ####################### Transformer Encoder\n        # Running Layer Normalization, MSA and residual connection\n        out = tokens + self.msa(self.ln1(tokens))\n        \n        #Running Layer Normalization, MLP  and residual connection\n        out = out + self.enc_mlp(self.ln2(out))\n        ####################### End transformer\n        \n        #Getting the classification token only\n        out = out[:,0]\n        \n        return self.mlp(out)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T17:00:28.507793Z","iopub.execute_input":"2022-06-13T17:00:28.508180Z","iopub.status.idle":"2022-06-13T17:00:28.534866Z","shell.execute_reply.started":"2022-06-13T17:00:28.508148Z","shell.execute_reply":"2022-06-13T17:00:28.533931Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"model = BumbleBird(\n    input_shape = (3,154,154),\n    n_patches = 11,\n    hidden_d=8, \n    n_heads=2, \n    out_d=10\n)\nx=torch.rand(16,3,154,154)\nprint(model(x).shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T09:03:51.600175Z","iopub.execute_input":"2022-06-12T09:03:51.600693Z","iopub.status.idle":"2022-06-12T09:03:51.690327Z","shell.execute_reply.started":"2022-06-12T09:03:51.600649Z","shell.execute_reply":"2022-06-12T09:03:51.689449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main()","metadata":{"execution":{"iopub.status.busy":"2022-06-11T21:04:39.079963Z","iopub.execute_input":"2022-06-11T21:04:39.080345Z","iopub.status.idle":"2022-06-11T21:04:41.092113Z","shell.execute_reply.started":"2022-06-11T21:04:39.080313Z","shell.execute_reply":"2022-06-11T21:04:41.090881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Imports\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.optim import Adam\nfrom torch.nn import CrossEntropyLoss\n\nfrom torchvision import datasets, transforms\nfrom torchvision.transforms import ToTensor\nfrom torchvision.io import read_image\nfrom torchvision.datasets.mnist import MNIST\n\nimport numpy as np\nimport os\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nfrom collections import Counter","metadata":{"execution":{"iopub.status.busy":"2022-06-13T17:49:54.457730Z","iopub.execute_input":"2022-06-13T17:49:54.458113Z","iopub.status.idle":"2022-06-13T17:49:54.464946Z","shell.execute_reply.started":"2022-06-13T17:49:54.458082Z","shell.execute_reply":"2022-06-13T17:49:54.464003Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#Ahora de verdad. Modúlos\n################## Patch Embedding\nclass PatchEmbed(nn.Module):\n    \"\"\"Divide la imagen en partes y la asocia a una posición\n        Parametros\n        --------------\n        img_size: int\n            El tamaño de la imagen debe de ser un cuadrado para poder dividirlo. \n            Será necesario que la imagen sea escalada a un cuadrado para conseguirlo.\n            \n        patch_size: int\n            - Tamaño de cada una de las partes en las que se divide la imagen.\n            - También deben de ser cuadrados.\n            - Debe de cumplir que el tamaño de la imagen sea divisible por el\n                tamaño de las partes.\n            \n        in_chans: int\n            - Número de canales de la imagen (color).\n            - Por ejemplo, si es en esacala de grises debe de ser de valor 1 mientras que de ser\n                una imagen RGB deberá tener valor 3. En este caso in_chans es normalmente de valor\n                3.\n            \n        embed_dim: int\n            - Como de grande será el \"embedding\" de una parte de la imagen durante toda la red\n                neuronal.\n        \n        Atributos\n        --------------\n        n_patches: int\n            - Numero de partes (patches) en los que dividimos la imagen.\n        \n        proj: nn.Conv2d\n            - Capa convolucional para dividir la imagen y colocarle su embedding.\n    \"\"\"\n    def __init__(self, img_size, patch_size, in_chans=3, embed_dim=768):\n        super().__init__()\n        self.img_size = img_size\n        self.patch_size = patch_size\n        #Calculamos el número de partes de la imagen\n        assert img_size%patch_size == 0, f\"The size {patch_size} for the patches cant divide image size {img_size} into equal patches\"\n        self.n_patches = (img_size // patch_size)**2\n        \n        self.proj = nn.Conv2d(\n            in_chans,\n            embed_dim,\n            kernel_size=patch_size,\n            stride=patch_size,\n        )\n    \n    def forward(self, x):\n        \"\"\"Run foward pass.\n        Parametros\n        -------------\n        x: torch.Tensor --------> Shape `(n_sambles, in_chans, img_size, img_size)`.\n            - Es un batch de imágenes\n            - n_samples == batch_size, El número de ejemplos es el mismo al del tamaño del batch.\n            - img_size: Altura y anchura de la imagen, que al ser un cuadrado, es la misma.\n            \n        Rerturns\n        -------------\n        torch.Tensor ----------> Shape `(n_samples, n_patches, embed_dim)`.\n            - n_patches: parches en los que dividimos la imagen\n        \"\"\"\n        x = self.proj(x) #  (n_samples, embed_dim, n_patches ** 0.5, n_patches ** 0.5) Esto nos da un tensor de 4 dimensiones\n        x = x.flatten(2) # (n_samples, embed_dim, n_patches) Lo aplanamos en una sola dimensión\n        x = x.transpose(1,2) # (n_samples, n_patches, embed_dim) Adecuamos el tensor\n        \n        return x\n        ","metadata":{},"execution_count":null,"outputs":[]}]}