{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Imports\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.optim import Adam\nfrom torch.nn import CrossEntropyLoss\n\nfrom torchvision import datasets, transforms\nfrom torchvision.transforms import ToTensor\nfrom torchvision.io import read_image\nfrom torchvision.datasets.mnist import MNIST\n\nimport numpy as np\nimport os\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nfrom collections import Counter\n\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2022-06-24T22:33:26.381360Z","iopub.execute_input":"2022-06-24T22:33:26.382240Z","iopub.status.idle":"2022-06-24T22:33:26.985357Z","shell.execute_reply.started":"2022-06-24T22:33:26.382136Z","shell.execute_reply":"2022-06-24T22:33:26.984345Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#Ahora de verdad. Modúlos\n################## Patch Embedding\nclass PatchEmbed(nn.Module):\n    \"\"\"Divide la imagen en partes y la asocia a una posición\n        Parametros\n        --------------\n        img_size: int\n            - El tamaño de la imagen debe de ser un cuadrado para poder dividirlo. \n            - Será necesario que la imagen sea escalada a un cuadrado para conseguirlo.\n            \n        patch_size: int\n            - Tamaño de cada una de las partes en las que se divide la imagen.\n            - También deben de ser cuadrados.\n            - Debe de cumplir que el tamaño de la imagen sea divisible por el\n                tamaño de las partes.\n            \n        in_chans: int\n            - Número de canales de la imagen (color).\n            - Por ejemplo, si es en escala de grises debe de ser de valor 1 mientras que de ser\n                una imagen RGB deberá tener valor 3. En este caso in_chans es normalmente de valor\n                3.\n            \n        embed_dim: int\n            - Como de grande será el \"embedding\" de una parte de la imagen durante toda la red\n                neuronal.\n        \n        Atributos\n        --------------\n        n_patches: int\n            - Numero de partes (patches) en los que dividimos la imagen.\n        \n        proj: nn.Conv2d\n            - Capa convolucional para dividir la imagen y colocarle su embedding.\n    \"\"\"\n    def __init__(self, img_size, patch_size, in_chans=3, embed_dim=768):\n        super().__init__()\n        self.img_size = img_size\n        self.patch_size = patch_size\n        #Calculamos el número de partes de la imagen\n        assert img_size%patch_size == 0, f\"The size {patch_size} for the patches cant divide image size {img_size} into equal patches\"\n        self.n_patches = (img_size // patch_size)**2\n        \n        self.proj = nn.Conv2d(\n            in_chans,\n            embed_dim,\n            kernel_size=patch_size,\n            stride=patch_size,\n            device=device\n        )\n    \n    def forward(self, x):\n        \"\"\"Run foward pass.\n        Parametros\n        -------------\n        x: torch.Tensor --------> Shape `(n_samples, in_chans, img_size, img_size)`.\n            - Es un batch de imágenes\n            - n_samples == batch_size, El número de ejemplos es el mismo al del tamaño del batch.\n            - img_size: Altura y anchura de la imagen, que al ser un cuadrado, es la misma.\n            \n        Rerturns\n        -------------\n        torch.Tensor ----------> Shape `(n_samples, n_patches, embed_dim)`.\n            - n_patches: parches en los que dividimos la imagen.\n        \"\"\"\n        x = self.proj(x) #  (n_samples, embed_dim, n_patches ** 0.5, n_patches ** 0.5) Esto nos da un tensor de 4 dimensiones\n        x = x.flatten(2) # (n_samples, embed_dim, n_patches) Lo aplanamos en una sola dimensión\n        x = x.transpose(1,2) # (n_samples, n_patches, embed_dim) Adecuamos el tensor\n        \n        return x\n\n################## Self-Attention Module\nclass Attention(nn.Module):\n    \"\"\"Attention mechanism\n    \n    Parameters\n    --------------\n    dim: int\n        - Dimensiones de la entrada y salida por cada token, debemos de hacer que ambos\n            valores tengan la misma dimensión.\n        \n    n_heads: int\n        - Número de cabezas de atención.\n        - Es necesario para el modelo trasnformer.\n    \n    qkv_bias: bool\n        - Si es verdadero, incluimos el bias a la query, clave y el valor de la proyección.\n    \n    attn_p: float\n        - Probabilidad de pérdida aplicada a la query, clave y los tensores valor.\n        \n    proj_p: float\n        - Probabilidad de pérdida aplicada al tensor de salida.\n        \n    Attributes\n    ---------------\n    scale: float\n        - Usado para normalizar el producto.\n    \n    qkv: nn.Linear\n        - Proyección linear de la query, clave y valor.\n        \n    proj: nn.Linear\n        - Mapeado linear que toma la secuencia de salida de todas las cabezas de atención y las\n            mapea a un nuevo espacio.\n    \n    att_drop, proj_drop: nn.Dropout\n        - Dropout layers\n    \"\"\"\n    def __init__(self, dim, n_heads=12, qkv_bias=True, attn_p=0., proj_p=0.):\n        super().__init__()\n        self.n_heads = n_heads\n        self.dim = dim\n        self.head_dim = dim // n_heads # El tensor de salida al unir las cabezas debería mantener la dimensión\n        self.scale = self.head_dim**-0.5 # Escala sugerida por el documento \"Attention is All you Need\" para evitar gradientes pequeños.\n        \n        self.qkv = nn.Linear(dim, dim*3, bias=qkv_bias, device=device) # Linear mapping que acepta una token de relleno y genera una query, key y valor\n        self.attn_drop = nn.Dropout(attn_p)\n        self.proj = nn.Linear(dim, dim, device=device) # Concatena las cabezas\n        self.proj_drop = nn.Dropout(proj_p)\n    \n    def forward(self, x):\n        \"\"\"Run forward pass -- Obsérvese que ambos tensores tienen la mismsa forma.\n        \n        Parameters\n        ------------\n        x: torch.Tensor ------------------> Shape `(n_samples, n_patches + 1, dim)`.\n        \n        Returns\n        ------------\n        torch.Tensor --------------------> Shape `(n_samples, n_patches + 1, dim)`.\n        \"\"\"\n        n_samples, n_tokens, dim = x.shape\n        \n        if dim!= self.dim:\n            raise ValueError\n        \n        qkv = self.qkv(x) # (n_samples, n_patches + 1, 3 * dim)\n        qkv = qkv.reshape(n_samples, n_tokens, 3, self.n_heads, self.head_dim) # (n_samples, n_patches + 1, 3, n_heads, self.head_dim)\n        qkv = qkv.permute(2,0,3,1,4) # (3, n_samples, n_heads, n_patches + 1, head_dim)\n        \n        q, k, v = qkv[0], qkv[1], qkv[2]\n        k_t = k.transpose(-2, -1) # (n_samples, n_heads, head_dim, n_patches + 1)\n        dp = (q @ k_t) * self.scale # (n_samples, n_heads, n_patches + 1, n_patches + 1)\n        attn = dp.softmax(dim=-1) # (n_samples, n_head, n_patches + 1, n_patches + 1) #Softmax function\n        attn = self.attn_drop(attn)\n        \n        weighted_avg = attn @ v # (n_samples, n_heads, n_patches + 1, head_dim)\n        weighted_avg = weighted_avg.transpose(1,2) # (n_samples, n_patches + 1, n_heads, head_dim)\n        weighted_avg = weighted_avg.flatten(2) # (n_samples, n_patches + 1, dim)\n        x = self.proj(weighted_avg) # (n_samples, n_patches + 1, dim)\n        x = self.proj_drop(x) # (n_samples, n_patches + 1, dim)\n        \n        return x\n\nclass MLP(nn.Module):\n    \"\"\"Multi-Layer Perceptron\n    \n    Parameters\n    -------------\n    in_features: int\n        - Número de inputs.\n    \n    hidden_features: int\n        - Número de nodos en la capa oculta. En nuestro caso tendrá 1 capa oculta.\n        \n    out_features: int\n        - Número de salidas.\n    \n    p: float:\n        - Probabilidad de pérdida.\n        \n    Attributes\n    -------------\n    fc: nn.Linear\n        - La primera capa linear.\n    \n    act: nn.GELU\n        - Función de activación GELU.\n        \n    fc2: nn.Linear\n        - La segunda capa linear.\n    \n    drop: nn.Dropout\n        - Capa de pérdida.\n    \"\"\"\n    def __init__(self, in_features, hidden_features, out_features, p = 0.):\n        super().__init__()\n        self.fc1 = nn.Linear(in_features, hidden_features, device=device)\n        self.act = nn.GELU()\n        self.fc2 = nn.Linear(hidden_features, out_features, device=device)\n        self.drop = nn.Dropout(p)\n        \n    def forward(self, x):\n        \"\"\"Run forward pass.\n        \n        Parameters\n        ------------\n        x: torch.Tensor ----------> Shape ´(n_samples, n_patches + 1, in_features)´\n        \n        Returns\n        ------------\n        torch.Tensor -------------> Shape `(n_samples, n_patches + 1, out_features)`\n        \"\"\"\n        x = self.fc1(x) # (n_samples, n_patches + 1, hidden_features)\n        x = self.act(x) # (n_samples, n_patches + 1, hidden_features)\n        x = self.drop(x) # (n_samples, n_patches + 1, hidden_features)\n        x = self.fc2(x) # (n_samples, n_patches + 1, hidden_features)\n        x = self.drop(x) # (n_samples, n_patches + 1, hidden_features)\n        \n        return x\n\nclass Block(nn.Module):\n    \"\"\"Transformer block\n    \n    Parameters\n    -----------\n    dim: int\n        - Dimensiónd el embedding\n    \n    n_heads: int\n        - Número de cabezas de atención\n    \n    mlp_ratio: float\n        - Determina el tamaño oculto de la dimensión de el módulo MLP con respecto a dim\n    \n    qkv_bias: bool\n        - Si es verdadero entonces incluimos el bias a las proyecciones de query, clave y valor.\n    \n    p, attn_p: float\n        - Probabilidad de pérdida\n    \n    Attributes\n    -----------\n    norm1, norm2: LayerNorm\n        - Módulo LayerNorm\n        \n    attn: Attention\n        - Módulo Attention\n    \n    mlp: MLP\n        - Módulo MLP\n    \"\"\"\n    def __init__(self, dim, n_heads, mlp_ratio=4.0, qkv_bias = True, p = 0., attn_p = 0.):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(dim, eps = 1e-6, device=device)\n        self.attn = Attention(dim, n_heads=n_heads,qkv_bias=qkv_bias, attn_p=attn_p, proj_p=p)\n        self.norm2 = nn.LayerNorm(dim, eps=1e-6, device=device)\n        hidden_features = int(dim*mlp_ratio)\n        self.mlp = MLP(in_features=dim, hidden_features = hidden_features, out_features = dim)\n        \n    def forward(self, x):\n        \"\"\"Run forward pass.\n        \n        Parameters\n        -----------\n        x: torch.Tensor --------> Shape `(n_samples,n_patches+1,dim)`\n        \n        Returns\n        -----------\n        torch.Tensor ----------> Shape`(n_samples, n_patches + 1, dim)`\n        \"\"\"\n        x = x + self.attn(self.norm1(x)) # Le sumamos un bloque residual\n        x = x + self.mlp(self.norm2(x))\n        \n        return x\n\nclass  VisionTransformer(nn.Module):\n    \"\"\"Simplified implementation of the Vision transformer.\n    \n    Parameters\n    -----------\n    img_size: int\n        - Altura y anchura de la imagen, que deben de ser iguales.\n    \n    patch_size: int\n        - Altura y anchura de las partes (tokens) en las que dividimos la imagen, de nuevo deben ser iguales.\n    \n    in_chans: int\n        - Número de canales para el input.\n        \n    n_classes: int\n        - Número de clases.\n        \n    embed_dim: int\n        - Dimensionalidad de los embeddings para cada token o parte de la imagen.\n        \n    deph: int\n        - Número de bloques del transformer.\n    \n    n_heads: int\n        - Número de cabezas de atención.\n    \n    mlp_ratio: float\n        - Determina la dimensión oculta para el módulo MLP\n        \n    qkv_bias: bool\n        - Si es true, entonces incluímos el bias a las proyecciones de la query, clave y valor.\n        \n    p, attn_p: float\n        - Probabilidad de pérdida.\n        \n    Attributes\n    -----------\n    patch_embed: PatchEmbed\n        - Instancia de PatchEmbed\n        - Es la primera capa de nuestra red.\n    \n    cls_token: nn.Parameter\n        - Parámetro con posibilidad de aprendizaje que representa la primera token en la secuencia.\n        - Tiene tantos elementos como el tamaño de `embed_dim`.\n    \n    pos_emb: nn.Parameter\n        - Envoltura posicional de la token cls y todas las partes\n        - Dónde exactamente está colocada esa token en la imagen.\n        - Tiene tantos elementos como `(n_patches + 1) * embed_dim`\n        \n    pos_drop: nn.Dropout\n        - Capa de pérdida.\n        \n    blocks: nn.ModuleList\n        - Lista de módulos `Block`.\n        \n    norm: nn.LayerNorm\n        - Normalización de las capas.\n    \"\"\"\n    def __init__(self, img_size = 384, patch_size=16, in_chans=3, n_classes=1000, embed_dim=768, deph=12,\n                n_heads=12,mlp_ratio=4., qkv_bias=True, p=0., attn_p = 0.,):\n        super().__init__()\n        \n        self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n        self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1,1+self.patch_embed.n_patches, embed_dim)).to(device)\n        self.pos_drop = nn.Dropout(p=p)\n        # Enconder del transformer, aunque cada bloque tenga los mismos parámetros, sus parámetros de aprendizaje serán diferentes.\n        self.blocks = nn.ModuleList([Block(dim=embed_dim, n_heads=n_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, p=p, attn_p=attn_p) for _ in range(deph)])\n        # Capa de normalización y Linear Mapping\n        self.norm = nn.LayerNorm(embed_dim, eps=1e-6, device=device)\n        self.head = nn.Linear(embed_dim, n_classes, device=device)\n    \n    def forward(self, x):\n        \"\"\"Run the forward pass\n        \n        Parametros\n        -------------\n        x: torch.Tensor ----------> Shape `(n_samples, in_chans, img_size, img_size)`.\n            - Un batch de imágenes.\n        \n        Returns\n        -------------\n        logits: torch.Tensor\n            - Logits over all the classes -> `(n_samples, n_classes)`\n        \n        \"\"\"\n       \n        n_samples = x.shape[0]\n        x = self.patch_embed(x) # Tomamos las imágenes de imput y las transformamos en patch embeddings\n        cls_token = self.cls_token.expand(n_samples, -1,-1).to(device) # (n_samples, 1, embed_dim)\n        x = torch.cat((cls_token, x), dim=1).to(device) # (n_samples, 1 + n_patches, embed_dim)\n        x = x + self.pos_embed # (n_samples, 1 + n_patches, embed_dim), Añadimos los embeddings posicionales que aprendimos\n        x = self.pos_drop(x) # Aplicamos la pérdida\n        \n        # Definimos todos los bloques de nuestro codificador.\n        for block in self.blocks:\n            x = block(x)\n        \n        # Normalizamos las capas\n        x = self.norm(x)\n        \n        cls_token_final = x[:,0] # Solo seleccionamos el embedding de clase\n        x = self.head(cls_token_final)\n        \n        # Esperamos que este embedding de `significado` a la imagen completa.\n        return x\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-06-24T22:33:28.286587Z","iopub.execute_input":"2022-06-24T22:33:28.287155Z","iopub.status.idle":"2022-06-24T22:33:28.329152Z","shell.execute_reply.started":"2022-06-24T22:33:28.287122Z","shell.execute_reply":"2022-06-24T22:33:28.328275Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def main(model = None, testing = False):\n    \n    transform = transforms.Compose([transforms.Resize(108),\n                                 transforms.CenterCrop(108),\n                                 transforms.ToTensor()])\n    dataset = datasets.ImageFolder('../input/transformer1dataset/birds/birds', transform=transform)\n\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n    \n    train_loader = DataLoader(train_dataset, shuffle=False, batch_size=16)\n    test_loader = DataLoader(test_dataset, shuffle=False, batch_size=16)\n    \n    #Shared parameters\n    N_EPOCHS = 5 # Number of epochs\n    criterion = CrossEntropyLoss()\n    if(model == None):\n        #Model and training options\n        model = VisionTransformer(img_size=108, patch_size=12,n_classes=400,deph=6,n_heads=6\n                                ,mlp_ratio=8.0)\n        LR = 0.01 #Learning rate\n    \n        #Training\n        optimizer = Adam(model.parameters(), lr = LR)\n        train(N_EPOCHS, train_loader,criterion, optimizer, model)\n        PATH = './selected_model.pth'\n        torch.save(model.state_dict(), PATH)\n        \n    if(testing == True):\n        #for t in range(N_EPOCHS):\n        #    print(f\"Epoch {t+1}\\n-----------------------------------\")\n        #    test(test_loader, model, criterion)\n        check_accuracy(test_loader, model)\n        print (\"Testing done!\")  ","metadata":{"execution":{"iopub.status.busy":"2022-06-24T23:06:31.900192Z","iopub.execute_input":"2022-06-24T23:06:31.900695Z","iopub.status.idle":"2022-06-24T23:06:31.913687Z","shell.execute_reply.started":"2022-06-24T23:06:31.900654Z","shell.execute_reply":"2022-06-24T23:06:31.912755Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def train(n_epochs, trainloader,criterion, optimizer, net):\n    for epoch in range(n_epochs):  # loop over the dataset multiple times\n\n        running_loss = 0.0\n        for i, data in enumerate(trainloader):\n            # get the inputs; data is a list of [inputs, labels]\n            inputs, labels = dataq\n            inputs, labels = inputs.to(device), labels.to(device)\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            # forward + backward + optimize\n            outputs = net(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            # print statistics\n            running_loss += loss.item()\n            if i % 500 == 499:    # print every 500 mini-batches\n                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n                running_loss = 0.0\n\n    print('Finished Training')\n\ndef test(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    model.to(device)\n    model.eval()\n    test_loss, correct = 0, 0\n    with torch.no_grad():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n    \ndef check_accuracy(loader, model):\n    num_correct = 0\n    num_samples = 0\n    model.eval()\n\n    for x, y in loader:\n        x = x.to(device=device)\n        y = y.to(device=device)\n\n        scores = model(x)\n        _, predictions = scores.max(1)\n        num_correct += (predictions == y).sum()\n        num_samples += predictions.size(0)\n    print(f'Got {num_correct} / {num_samples} with accuracy  \\ {float(num_correct)/float(num_samples)*100:.2f}')\n    model.train()","metadata":{"execution":{"iopub.status.busy":"2022-06-24T23:13:14.197217Z","iopub.execute_input":"2022-06-24T23:13:14.197593Z","iopub.status.idle":"2022-06-24T23:13:14.210988Z","shell.execute_reply.started":"2022-06-24T23:13:14.197561Z","shell.execute_reply":"2022-06-24T23:13:14.210266Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"#Training\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device} device\")\nmain(testing=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Testing\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = VisionTransformer(img_size=108, patch_size=9,n_classes=400,deph=6,n_heads=6)\npre_trained_path=\"../input/modeltransformer2/selected_model.pth\"\nstate_dict = torch.load(pre_trained_path)\nmodel.load_state_dict(state_dict)\nmodel.eval()\nprint(f'model {pre_trained_path} loaded')\nmain(model=model, testing=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T23:13:16.917164Z","iopub.execute_input":"2022-06-24T23:13:16.917553Z","iopub.status.idle":"2022-06-24T23:14:59.858283Z","shell.execute_reply.started":"2022-06-24T23:13:16.917521Z","shell.execute_reply":"2022-06-24T23:14:59.857470Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"model ../input/modeltransformer2/selected_model.pth loaded\nGot 117 / 11678 with accuracy  \\ 1.00\nTesting done!\n","output_type":"stream"}]},{"cell_type":"code","source":"# Test just one bird\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ntransform = transforms.Compose([transforms.Resize(108),\n                                 transforms.CenterCrop(108)])\ndataset = datasets.ImageFolder('../input/transformer1dataset/birds/birds', transform=transform)\nmodel = VisionTransformer(img_size=108, patch_size=12,n_classes=400,deph=6,n_heads=6)\nimg = Image.open(\"../input/transformer1dataset/birds/birds/RED FODY/001.jpg\")\nimg_normalized = transform(img)\n\nimg = (np.array(img_normalized) / 36)-1 # En rango -1, 1. Para el \"36\" se divide el tamaño de la imagen entre 3\nprint(img.shape)\ninp = torch.from_numpy(img).permute(2,0,1).unsqueeze(0).to(torch.float32).to(device)\n\nprint(inp.shape)\nlogits = model(inp)\nprobs = torch.nn.functional.softmax(logits, dim=1)\n\nk = 30\ntop_probs, top_ixs = probs[0].topk(k)\n\ndict = dataset.class_to_idx\nkey_list = list(dict.keys())\nval_list = list(dict.values())\n\n\nfor i, (ix_, prob_) in enumerate(zip(top_ixs, top_probs)):\n    ix = ix_.item()\n    prob = prob_.item()\n    position = val_list.index(ix)\n    cls = key_list[position].strip()\n    print(f\"{i}: {cls:<45} --- {prob:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2022-06-24T23:56:51.668274Z","iopub.execute_input":"2022-06-24T23:56:51.669302Z","iopub.status.idle":"2022-06-24T23:56:52.187319Z","shell.execute_reply.started":"2022-06-24T23:56:51.669254Z","shell.execute_reply":"2022-06-24T23:56:52.186469Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"(108, 108, 3)\ntorch.Size([1, 3, 108, 108])\n0: SATYR TRAGOPAN                                --- 0.0107\n1: CRANE HAWK                                    --- 0.0085\n2: CAPE LONGCLAW                                 --- 0.0075\n3: HAWAIIAN GOOSE                                --- 0.0072\n4: BANDED PITA                                   --- 0.0072\n5: ROYAL FLYCATCHER                              --- 0.0069\n6: BAIKAL TEAL                                   --- 0.0067\n7: STRAWBERRY FINCH                              --- 0.0064\n8: PURPLE FINCH                                  --- 0.0062\n9: SCARLET MACAW                                 --- 0.0059\n10: MASKED BOOBY                                  --- 0.0057\n11: WATTLED LAPWING                               --- 0.0054\n12: RED BROWED FINCH                              --- 0.0054\n13: BOBOLINK                                      --- 0.0054\n14: JANDAYA PARAKEET                              --- 0.0052\n15: GOLDEN CHEEKED WARBLER                        --- 0.0052\n16: GREAT GRAY OWL                                --- 0.0052\n17: BAY-BREASTED WARBLER                          --- 0.0051\n18: NORTHERN PARULA                               --- 0.0050\n19: INDIAN BUSTARD                                --- 0.0050\n20: CRIMSON CHAT                                  --- 0.0050\n21: SUPERB STARLING                               --- 0.0049\n22: TAILORBIRD                                    --- 0.0049\n23: BALTIMORE ORIOLE                              --- 0.0048\n24: BEARDED REEDLING                              --- 0.0047\n25: SHOEBILL                                      --- 0.0046\n26: YELLOW HEADED BLACKBIRD                       --- 0.0046\n27: HAWFINCH                                      --- 0.0046\n28: GREEN BROADBILL                               --- 0.0046\n29: BALI STARLING                                 --- 0.0045\n","output_type":"stream"}]},{"cell_type":"code","source":"# Test and write submission_test.csv\nimport csv\nimport os\nfrom natsort import natsorted\nfrom torch.utils.data import Dataset\n\nclass NoClassDataset(Dataset):\n    def __init__(self, main_dir, transform):\n        self.main_dir = main_dir\n        self.transform = transform\n        all_imgs = os.listdir(main_dir)\n        self.total_imgs = natsorted(all_imgs)\n\n    def __len__(self):\n        return len(self.total_imgs)\n\n    def __getitem__(self, idx):\n        img_loc = os.path.join(self.main_dir, self.total_imgs[idx])\n        image = Image.open(img_loc).convert(\"RGB\")\n        tensor_image = self.transform(image)\n        tensor_image = tensor_image.to(device)\n        return tensor_image\n    \n    def getFileName(self, idx):\n        img_loc = os.path.join(self.main_dir, self.total_imgs[idx])\n        filename = os.path.basename(img_loc)\n        return filename\n\n#Creamos el csv\n\nwith open('submission.csv', 'w') as file:\n    data = [\"Id\", \"Category\"]\n    writer = csv.writer(file)\n    writer.writerow(data)\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    transform = transforms.Compose([transforms.Resize(108),\n                                     transforms.CenterCrop(108),transforms.ToTensor()])\n    dataset = datasets.ImageFolder('../input/transformer1dataset/birds/birds', transform=transform)\n    model = VisionTransformer(img_size=108, patch_size=12,n_classes=400,deph=6,n_heads=6)\n\n    submissions = NoClassDataset('../input/transformer1dataset/submission_test/submission_test', transform=transform)\n    submissions_loader = DataLoader(submissions , batch_size=16, shuffle=False)\n\n    dict = dataset.class_to_idx\n    key_list = list(dict.keys())\n    val_list = list(dict.values())\n    \n    for epoch in range(16): #Necesario para iterar las 2000, La longitud del dataset es batch_size x num_epochs\n        for idx, img_normalized in enumerate(submissions_loader):\n            logits = model(img_normalized)\n            probs = torch.nn.functional.softmax(logits, dim=1)\n\n\n            k = 1\n            top_prob, top_ix = probs[0].topk(k)\n            ix = top_ix.item()\n            prob = top_prob.item()\n            position = val_list.index(top_ix)\n            cls = key_list[position].strip()\n            #import pdb; pdb.set_trace()\n            row = [submissions.getFileName(idx), cls]\n            writer.writerow(row)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T00:17:58.700694Z","iopub.execute_input":"2022-06-25T00:17:58.701293Z","iopub.status.idle":"2022-06-25T00:20:09.803799Z","shell.execute_reply.started":"2022-06-25T00:17:58.701255Z","shell.execute_reply":"2022-06-25T00:20:09.802800Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}